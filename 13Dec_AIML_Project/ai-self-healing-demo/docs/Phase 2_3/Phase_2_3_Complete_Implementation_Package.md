# PHASE 2 & 3 COMPLETE IMPLEMENTATION PACKAGE
## Ready-to-Deploy Self-Healing Platform

**Student:** Piyush Ashokkumar Raval  
**Timeline:** January - March 2026  
**Status:** Production-Ready Implementation

---

## üì¶ PACKAGE CONTENTS

This package contains everything needed to implement Phase 2 (Functional Prototype) and Phase 3 (Deployment & Testing):

### Phase 2 Components:
1. ‚úÖ JMeter Load Testing (3 test plans + automation)
2. ‚úÖ Enhanced Chaos Engineering (10 scenarios)
3. ‚úÖ Kubernetes Deployment (Local + Cloud)
4. ‚úÖ AWS Cloud Integration (EKS, Auto Scaling, CloudWatch)
5. ‚úÖ Azure Alternative (Optional)

### Phase 3 Components:
6. ‚úÖ CI/CD Pipeline (GitHub Actions)
7. ‚úÖ Automated Testing Suite (Unit, Integration, E2E)
8. ‚úÖ Monitoring & Observability (Prometheus + Grafana)
9. ‚úÖ Performance Testing & Validation

---

## üöÄ QUICK START GUIDE

### Prerequisites Checklist

```bash
# Check Python
python --version  # Should be 3.9+

# Check Docker
docker --version
docker ps

# Check kubectl
kubectl version --client

# Check AWS CLI (for cloud deployment)
aws --version
aws sts get-caller-identity

# Check Git
git --version
```

### Installation Steps

```bash
# 1. Clone/Navigate to project
cd ai-self-healing-platform

# 2. Install all dependencies
pip install -r requirements-dev.txt

# 3. Download JMeter
./scripts/install_jmeter.sh

# 4. Install Minikube (for local Kubernetes)
./scripts/install_minikube.sh

# 5. Verify installation
./scripts/verify_setup.sh
```

---

## üìÅ COMPLETE DIRECTORY STRUCTURE

```
ai-self-healing-platform/
‚îÇ
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ ci.yml                          # Continuous Integration
‚îÇ       ‚îú‚îÄ‚îÄ cd-staging.yml                  # Staging Deployment
‚îÇ       ‚îî‚îÄ‚îÄ cd-production.yml               # Production Deployment
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ main.py                         # FastAPI server (v13)
‚îÇ   ‚îú‚îÄ‚îÄ ml/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ anomaly_detector.py             # ML models
‚îÇ   ‚îú‚îÄ‚îÄ orchestrator/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ self_healing.py                 # Self-healing logic
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_collector.py            # Metrics collection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prometheus_exporter.py          # Prometheus integration
‚îÇ   ‚îú‚îÄ‚îÄ cloud/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_integration.py              # AWS SDK integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ azure_integration.py            # Azure SDK integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kubernetes_integration.py       # K8s Python client
‚îÇ   ‚îú‚îÄ‚îÄ testing/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jmeter_integration.py           # JMeter automation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chaos_orchestrator.py           # Chaos engineering
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ logger.py                       # Logging utilities
‚îÇ       ‚îî‚îÄ‚îÄ config.py                       # Configuration management
‚îÇ
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_anomaly_detector.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_self_healing.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_metrics_collector.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_integration.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_ml_pipeline.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_end_to_end.py
‚îÇ   ‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_complete_workflow.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_chaos_scenarios.py
‚îÇ   ‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_load_scenarios.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_stress_scenarios.py
‚îÇ   ‚îî‚îÄ‚îÄ conftest.py                         # Pytest fixtures
‚îÇ
‚îú‚îÄ‚îÄ kubernetes/
‚îÇ   ‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hpa.yaml                        # Horizontal Pod Autoscaler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdb.yaml                        # Pod Disruption Budget
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ networkpolicy.yaml
‚îÇ   ‚îú‚îÄ‚îÄ overlays/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kustomization.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ production/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ kustomization.yaml
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ prometheus.yaml
‚îÇ       ‚îú‚îÄ‚îÄ grafana.yaml
‚îÇ       ‚îî‚îÄ‚îÄ servicemonitor.yaml
‚îÇ
‚îú‚îÄ‚îÄ aws/
‚îÇ   ‚îú‚îÄ‚îÄ eks-cluster-config.yaml             # EKS cluster definition
‚îÇ   ‚îú‚îÄ‚îÄ cluster-autoscaler.yaml             # AWS autoscaler
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml                        # ALB ingress
‚îÇ   ‚îú‚îÄ‚îÄ iam-policy.json                     # IAM policies
‚îÇ   ‚îî‚îÄ‚îÄ cloudformation/
‚îÇ       ‚îú‚îÄ‚îÄ vpc-stack.yaml
‚îÇ       ‚îî‚îÄ‚îÄ eks-stack.yaml
‚îÇ
‚îú‚îÄ‚îÄ azure/
‚îÇ   ‚îú‚îÄ‚îÄ aks-cluster-config.yaml             # AKS cluster definition
‚îÇ   ‚îú‚îÄ‚îÄ arm-templates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vnet.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ aks.json
‚îÇ   ‚îî‚îÄ‚îÄ azure-pipelines.yml                 # Azure DevOps pipeline
‚îÇ
‚îú‚îÄ‚îÄ jmeter/
‚îÇ   ‚îú‚îÄ‚îÄ test-plans/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normal_load_test.jmx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stress_test.jmx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spike_test.jmx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ endurance_test.jmx
‚îÇ   ‚îú‚îÄ‚îÄ results/                            # Auto-generated results
‚îÇ   ‚îú‚îÄ‚îÄ reports/                            # HTML reports
‚îÇ   ‚îî‚îÄ‚îÄ run_all_tests.py                    # Test automation
‚îÇ
‚îú‚îÄ‚îÄ chaos/
‚îÇ   ‚îú‚îÄ‚îÄ scenarios/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cpu_spike.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ memory_leak.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network_latency.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service_crash.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cascade_failure.py
‚îÇ   ‚îú‚îÄ‚îÄ chaos_config.yaml                   # Chaos configuration
‚îÇ   ‚îî‚îÄ‚îÄ run_chaos_tests.py                  # Chaos automation
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml                  # Prometheus config
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts.yml                      # Alert rules
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recording-rules.yml
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ platform-overview.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml-metrics.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ kubernetes-metrics.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ provisioning/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ datasources.yaml
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ dashboards.yaml
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager/
‚îÇ       ‚îî‚îÄ‚îÄ alertmanager.yml
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ install_jmeter.sh
‚îÇ   ‚îú‚îÄ‚îÄ install_minikube.sh
‚îÇ   ‚îú‚îÄ‚îÄ verify_setup.sh
‚îÇ   ‚îú‚îÄ‚îÄ setup_aws_eks.sh
‚îÇ   ‚îú‚îÄ‚îÄ setup_azure_aks.sh
‚îÇ   ‚îú‚îÄ‚îÄ deploy_to_kubernetes.sh
‚îÇ   ‚îú‚îÄ‚îÄ deploy_to_aws.sh
‚îÇ   ‚îú‚îÄ‚îÄ deploy_to_azure.sh
‚îÇ   ‚îú‚îÄ‚îÄ run_all_tests.sh
‚îÇ   ‚îú‚îÄ‚îÄ backup_and_restore.sh
‚îÇ   ‚îî‚îÄ‚îÄ cleanup.sh
‚îÇ
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                          # Main application
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile.test                     # Testing image
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml                  # Local development
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.prod.yml             # Production setup
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ development.yaml
‚îÇ   ‚îú‚îÄ‚îÄ staging.yaml
‚îÇ   ‚îú‚îÄ‚îÄ production.yaml
‚îÇ   ‚îî‚îÄ‚îÄ secrets.example.yaml
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ INSTALLATION.md
‚îÇ   ‚îú‚îÄ‚îÄ KUBERNETES_DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ AWS_DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ AZURE_DEPLOYMENT.md
‚îÇ   ‚îú‚îÄ‚îÄ CI_CD_SETUP.md
‚îÇ   ‚îú‚îÄ‚îÄ TESTING_GUIDE.md
‚îÇ   ‚îî‚îÄ‚îÄ TROUBLESHOOTING.md
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                        # Production dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt                    # Development dependencies
‚îú‚îÄ‚îÄ pytest.ini                              # Pytest configuration
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Makefile                                # Common commands
‚îî‚îÄ‚îÄ README.md
```

---

## üîß PHASE 2: IMPLEMENTATION GUIDE

### 2.1 JMeter Load Testing

#### Step 1: Install JMeter

**File: `scripts/install_jmeter.sh`**

```bash
#!/bin/bash
set -e

echo "üöÄ Installing Apache JMeter"
echo "============================"

JMETER_VERSION="5.6.3"
INSTALL_DIR="${HOME}/jmeter"

# Detect OS
if [[ "$OSTYPE" == "linux-gnu"* ]]; then
    OS="linux"
elif [[ "$OSTYPE" == "darwin"* ]]; then
    OS="mac"
else
    echo "‚ùå Unsupported OS. Please install JMeter manually."
    exit 1
fi

echo "üì¶ Downloading JMeter ${JMETER_VERSION}..."
cd /tmp
wget https://dlcdn.apache.org/jmeter/binaries/apache-jmeter-${JMETER_VERSION}.tgz
tar -xzf apache-jmeter-${JMETER_VERSION}.tgz

echo "üìÅ Installing to ${INSTALL_DIR}..."
mkdir -p ${INSTALL_DIR}
mv apache-jmeter-${JMETER_VERSION} ${INSTALL_DIR}/

echo "üîß Setting up environment..."
echo "export JMETER_HOME=${INSTALL_DIR}/apache-jmeter-${JMETER_VERSION}" >> ~/.bashrc
echo "export PATH=\$PATH:\$JMETER_HOME/bin" >> ~/.bashrc

# For current session
export JMETER_HOME=${INSTALL_DIR}/apache-jmeter-${JMETER_VERSION}
export PATH=$PATH:$JMETER_HOME/bin

echo "‚úÖ JMeter installed successfully!"
echo "üìù Run: source ~/.bashrc"
echo "üß™ Test: jmeter --version"
```

#### Step 2: Create JMeter Test Plans

**File: `jmeter/run_all_tests.py`**

```python
#!/usr/bin/env python3
"""
Comprehensive JMeter Test Runner
Executes all load test scenarios and generates reports
"""

import subprocess
import os
import sys
import json
from pathlib import Path
from datetime import datetime
import argparse

class JMeterTestRunner:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.jmeter_home = os.getenv('JMETER_HOME')
        self.test_plans_dir = Path('jmeter/test-plans')
        self.results_dir = Path('jmeter/results')
        self.reports_dir = Path('jmeter/reports')
        
        # Create directories
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Validate JMeter installation
        if not self.jmeter_home:
            print("‚ùå JMETER_HOME not set. Please install JMeter first.")
            sys.exit(1)
        
        self.jmeter_bin = Path(self.jmeter_home) / 'bin' / 'jmeter'
        if not self.jmeter_bin.exists():
            print(f"‚ùå JMeter binary not found at {self.jmeter_bin}")
            sys.exit(1)
    
    def run_test(self, test_plan, test_name, threads=100, duration=300):
        """Execute a single JMeter test"""
        print(f"\n{'='*70}")
        print(f"üß™ Running Test: {test_name}")
        print(f"{'='*70}")
        print(f"üìã Test Plan: {test_plan}")
        print(f"üë• Threads: {threads}")
        print(f"‚è±Ô∏è  Duration: {duration}s")
        print(f"üéØ Target: {self.base_url}")
        print()
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = self.results_dir / f"{test_name}_{timestamp}.jtl"
        report_dir = self.reports_dir / f"{test_name}_{timestamp}"
        log_file = self.results_dir / f"{test_name}_{timestamp}.log"
        
        # JMeter command
        cmd = [
            str(self.jmeter_bin),
            '-n',  # Non-GUI mode
            '-t', str(test_plan),  # Test plan
            '-l', str(results_file),  # Results log
            '-j', str(log_file),  # JMeter log
            '-e',  # Generate report
            '-o', str(report_dir),  # Report output
            '-Jthreads=' + str(threads),
            '-Jduration=' + str(duration),
            '-Jurl=' + self.base_url
        ]
        
        start_time = datetime.now()
        
        try:
            print("üöÄ Starting test execution...")
            result = subprocess.run(
                cmd,
                check=True,
                capture_output=True,
                text=True
            )
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            print(f"\n‚úÖ Test completed successfully!")
            print(f"‚è±Ô∏è  Execution time: {execution_time:.2f}s")
            print(f"üìä Results: {results_file}")
            print(f"üìà Report: {report_dir}/index.html")
            
            # Analyze results
            stats = self.analyze_results(results_file)
            
            return {
                'test_name': test_name,
                'status': 'success',
                'execution_time': execution_time,
                'results_file': str(results_file),
                'report_dir': str(report_dir),
                'statistics': stats
            }
            
        except subprocess.CalledProcessError as e:
            print(f"\n‚ùå Test failed!")
            print(f"Error: {e.stderr}")
            
            return {
                'test_name': test_name,
                'status': 'failed',
                'error': str(e)
            }
    
    def analyze_results(self, results_file):
        """Analyze JMeter results CSV"""
        try:
            import pandas as pd
            
            df = pd.read_csv(results_file)
            
            stats = {
                'total_requests': len(df),
                'successful': len(df[df['success'] == True]),
                'failed': len(df[df['success'] == False]),
                'error_rate': (len(df[df['success'] == False]) / len(df) * 100),
                'avg_response_time': df['elapsed'].mean(),
                'min_response_time': df['elapsed'].min(),
                'max_response_time': df['elapsed'].max(),
                'p50': df['elapsed'].quantile(0.50),
                'p90': df['elapsed'].quantile(0.90),
                'p95': df['elapsed'].quantile(0.95),
                'p99': df['elapsed'].quantile(0.99)
            }
            
            print("\nüìä Test Statistics:")
            print(f"  Total Requests: {stats['total_requests']}")
            print(f"  Success Rate: {100 - stats['error_rate']:.2f}%")
            print(f"  Avg Response Time: {stats['avg_response_time']:.2f}ms")
            print(f"  P95 Response Time: {stats['p95']:.2f}ms")
            print(f"  P99 Response Time: {stats['p99']:.2f}ms")
            
            return stats
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not analyze results: {e}")
            return {}
    
    def run_all_tests(self):
        """Run complete test suite"""
        print("\n" + "="*70)
        print("üöÄ JMETER TEST SUITE EXECUTION")
        print("="*70)
        
        test_scenarios = [
            {
                'name': 'normal_load',
                'plan': self.test_plans_dir / 'normal_load_test.jmx',
                'threads': 100,
                'duration': 300
            },
            {
                'name': 'stress_test',
                'plan': self.test_plans_dir / 'stress_test.jmx',
                'threads': 500,
                'duration': 300
            },
            {
                'name': 'spike_test',
                'plan': self.test_plans_dir / 'spike_test.jmx',
                'threads': 1000,
                'duration': 120
            }
        ]
        
        results = []
        
        for scenario in test_scenarios:
            if not scenario['plan'].exists():
                print(f"\n‚ö†Ô∏è  Test plan not found: {scenario['plan']}")
                print(f"   Skipping {scenario['name']}...")
                continue
            
            result = self.run_test(
                scenario['plan'],
                scenario['name'],
                scenario['threads'],
                scenario['duration']
            )
            
            results.append(result)
            
            # Wait between tests
            if scenario != test_scenarios[-1]:
                print(f"\n‚è∏Ô∏è  Waiting 30s before next test...")
                import time
                time.sleep(30)
        
        # Print summary
        self.print_summary(results)
        
        # Save summary
        self.save_summary(results)
        
        return results
    
    def print_summary(self, results):
        """Print test execution summary"""
        print("\n" + "="*70)
        print("üìä TEST EXECUTION SUMMARY")
        print("="*70)
        
        for result in results:
            status_icon = "‚úÖ" if result['status'] == 'success' else "‚ùå"
            print(f"\n{status_icon} {result['test_name']}")
            
            if result['status'] == 'success':
                stats = result.get('statistics', {})
                print(f"  Success Rate: {100 - stats.get('error_rate', 0):.2f}%")
                print(f"  Avg Response: {stats.get('avg_response_time', 0):.2f}ms")
                print(f"  P95 Response: {stats.get('p95', 0):.2f}ms")
                print(f"  Report: {result['report_dir']}/index.html")
            else:
                print(f"  Error: {result.get('error', 'Unknown')}")
        
        # Overall status
        passed = sum(1 for r in results if r['status'] == 'success')
        total = len(results)
        
        print(f"\n{'='*70}")
        print(f"Overall: {passed}/{total} tests passed")
        print("="*70)
    
    def save_summary(self, results):
        """Save test summary to JSON"""
        summary_file = self.results_dir / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(summary_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nüíæ Summary saved: {summary_file}")

def main():
    parser = argparse.ArgumentParser(description='JMeter Test Runner')
    parser.add_argument('--url', default='http://localhost:8000', help='Target URL')
    parser.add_argument('--test', help='Run specific test (normal_load, stress_test, spike_test)')
    
    args = parser.parse_args()
    
    runner = JMeterTestRunner(base_url=args.url)
    
    if args.test:
        # Run specific test
        test_plan = runner.test_plans_dir / f'{args.test}.jmx'
        if test_plan.exists():
            runner.run_test(test_plan, args.test)
        else:
            print(f"‚ùå Test plan not found: {test_plan}")
            sys.exit(1)
    else:
        # Run all tests
        runner.run_all_tests()

if __name__ == '__main__':
    main()
```

Make executable:
```bash
chmod +x jmeter/run_all_tests.py
```

### 2.2 Enhanced Chaos Engineering

**File: `chaos/run_chaos_tests.py`**

```python
#!/usr/bin/env python3
"""
Comprehensive Chaos Engineering Test Suite
"""

import asyncio
import sys
from pathlib import Path
import json
from datetime import datetime

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / 'src'))

from src.testing.chaos_orchestrator import AdvancedChaosEngine, ChaosScenario

async def run_chaos_suite():
    """Execute comprehensive chaos test suite"""
    print("="*70)
    print("üî• CHAOS ENGINEERING TEST SUITE")
    print("="*70)
    
    engine = AdvancedChaosEngine()
    
    # Define test scenarios
    scenarios = [
        {
            'name': 'CPU Spike',
            'scenario': ChaosScenario.CPU_SPIKE,
            'duration': 30,
            'intensity': 0.8
        },
        {
            'name': 'Memory Leak',
            'scenario': ChaosScenario.MEMORY_LEAK,
            'duration': 30,
            'intensity': 0.7
        },
        {
            'name': 'Network Latency',
            'scenario': ChaosScenario.NETWORK_LATENCY,
            'duration': 30,
            'intensity': 0.6
        },
        {
            'name': 'Service Crash',
            'scenario': ChaosScenario.SERVICE_CRASH,
            'duration': 20,
            'intensity': 1.0
        },
        {
            'name': 'Cascade Failure',
            'scenario': ChaosScenario.CASCADE_FAILURE,
            'duration': 45,
            'intensity': 0.8
        }
    ]
    
    results = []
    
    for test in scenarios:
        print(f"\n{'='*70}")
        print(f"üî• Test: {test['name']}")
        print(f"{'='*70}")
        
        result = await engine.inject_chaos(
            test['scenario'],
            duration=test['duration'],
            intensity=test['intensity']
        )
        
        results.append({
            'name': test['name'],
            'result': result
        })
        
        # Recovery time
        print(f"\n‚è∏Ô∏è  Recovery period (30s)...")
        await asyncio.sleep(30)
    
    # Save results
    output_dir = Path('chaos/results')
    output_dir.mkdir(parents=True, exist_ok=True)
    
    output_file = output_dir / f"chaos_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\n{'='*70}")
    print(f"üìä CHAOS TEST SUMMARY")
    print(f"{'='*70}")
    print(f"Total Scenarios: {len(results)}")
    print(f"Results saved: {output_file}")
    print(f"{'='*70}")

if __name__ == '__main__':
    asyncio.run(run_chaos_suite())
```

### 2.3 Kubernetes Deployment

**File: `scripts/deploy_to_kubernetes.sh`**

```bash
#!/bin/bash
set -e

echo "‚ò∏Ô∏è  Deploying to Kubernetes"
echo "==========================="

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Configuration
NAMESPACE="self-healing-platform"
CONTEXT=$(kubectl config current-context)

echo -e "${YELLOW}Current context: ${CONTEXT}${NC}"
echo -e "${YELLOW}Namespace: ${NAMESPACE}${NC}"
echo ""

# Confirm deployment
read -p "Continue with deployment? (y/n) " -n 1 -r
echo
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Deployment cancelled."
    exit 1
fi

# Step 1: Build Docker image
echo -e "${YELLOW}Step 1/7: Building Docker image...${NC}"
docker build -t self-healing-platform:latest .

# Step 2: Load image into cluster (for Minikube)
if [[ $CONTEXT == *"minikube"* ]]; then
    echo -e "${YELLOW}Step 2/7: Loading image into Minikube...${NC}"
    minikube image load self-healing-platform:latest
else
    echo -e "${YELLOW}Step 2/7: Skipping image load (not using Minikube)${NC}"
fi

# Step 3: Create namespace
echo -e "${YELLOW}Step 3/7: Creating namespace...${NC}"
kubectl apply -f kubernetes/base/namespace.yaml

# Step 4: Apply ConfigMap
echo -e "${YELLOW}Step 4/7: Applying ConfigMap...${NC}"
kubectl apply -f kubernetes/base/configmap.yaml

# Step 5: Deploy application
echo -e "${YELLOW}Step 5/7: Deploying application...${NC}"
kubectl apply -f kubernetes/base/deployment.yaml
kubectl apply -f kubernetes/base/service.yaml

# Step 6: Setup HPA
echo -e "${YELLOW}Step 6/7: Setting up Horizontal Pod Autoscaler...${NC}"
kubectl apply -f kubernetes/base/hpa.yaml

# Step 7: Wait for pods
echo -e "${YELLOW}Step 7/7: Waiting for pods to be ready...${NC}"
kubectl wait --for=condition=ready pod \
    -l app=self-healing-platform \
    -n ${NAMESPACE} \
    --timeout=300s

echo -e "${GREEN}‚úÖ Deployment successful!${NC}"
echo ""
echo "üìä Cluster Status:"
kubectl get all -n ${NAMESPACE}

echo ""
echo "üåê Access Options:"
if [[ $CONTEXT == *"minikube"* ]]; then
    echo "  minikube service self-healing-platform-service -n ${NAMESPACE}"
else
    EXTERNAL_IP=$(kubectl get svc self-healing-platform-service -n ${NAMESPACE} -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    if [ -z "$EXTERNAL_IP" ]; then
        echo "  kubectl port-forward svc/self-healing-platform-service 8000:80 -n ${NAMESPACE}"
    else
        echo "  http://${EXTERNAL_IP}"
    fi
fi

echo ""
echo "üìù Useful Commands:"
echo "  View pods:    kubectl get pods -n ${NAMESPACE}"
echo "  View logs:    kubectl logs -f <pod-name> -n ${NAMESPACE}"
echo "  Describe HPA: kubectl describe hpa -n ${NAMESPACE}"
echo "  Delete all:   kubectl delete namespace ${NAMESPACE}"
```

Make executable:
```bash
chmod +x scripts/deploy_to_kubernetes.sh
```

---

## üå©Ô∏è PHASE 3: CI/CD & TESTING

### 3.1 Complete Test Suite

**File: `tests/integration/test_complete_workflow.py`**

```python
"""
Complete Integration Test Suite
Tests entire platform workflow from metrics to healing
"""

import pytest
import requests
import time
import json

BASE_URL = "http://localhost:8000"

class TestCompleteWorkflow:
    """Test complete platform workflow"""
    
    @pytest.fixture(autouse=True)
    def setup(self):
        """Setup before each test"""
        # Wait for platform to be ready
        for _ in range(30):
            try:
                response = requests.get(f"{BASE_URL}/health", timeout=2)
                if response.status_code == 200:
                    break
            except:
                pass
            time.sleep(1)
        else:
            pytest.fail("Platform not ready after 30 seconds")
    
    def test_01_platform_health(self):
        """Test 1: Platform health check"""
        response = requests.get(f"{BASE_URL}/health")
        assert response.status_code == 200
        
        data = response.json()
        assert data['status'] == 'healthy'
        assert 'timestamp' in data
    
    def test_02_api_status(self):
        """Test 2: API status endpoint"""
        response = requests.get(f"{BASE_URL}/api/v1/status")
        assert response.status_code == 200
        
        data = response.json()
        assert 'health_score' in data
        assert 'ml_model_trained' in data
        assert data['health_score'] >= 0
        assert data['health_score'] <= 100
    
    def test_03_metrics_ingestion(self):
        """Test 3: Metrics ingestion"""
        metric = {
            'timestamp': '2026-01-01T12:00:00',
            'cpu_usage': 65.0,
            'memory_usage': 70.0,
            'response_time': 250.0,
            'error_rate': 1.5,
            'requests_per_sec': 120.0
        }
        
        response = requests.post(f"{BASE_URL}/api/v1/metrics", json=metric)
        assert response.status_code == 200
        
        data = response.json()
        assert data['status'] == 'accepted'
    
    def test_04_ml_model_training(self):
        """Test 4: ML model gets trained with sufficient data"""
        # Send 25 normal metrics
        for i in range(25):
            metric = {
                'timestamp': f'2026-01-01T12:{i:02d}:00',
                'cpu_usage': 50.0 + (i % 10),
                'memory_usage': 60.0 + (i % 10),
                'response_time': 200.0 + (i % 50),
                'error_rate': 1.0,
                'requests_per_sec': 100.0
            }
            requests.post(f"{BASE_URL}/api/v1/metrics", json=metric)
        
        time.sleep(2)
        
        # Check ML model status
        response = requests.get(f"{BASE_URL}/api/v1/status")
        data = response.json()
        assert data['ml_model_trained'] == True
    
    def test_05_anomaly_detection(self):
        """Test 5: Anomaly detection workflow"""
        # Send anomalous metric
        anomaly_metric = {
            'timestamp': '2026-01-01T13:00:00',
            'cpu_usage': 95.0,
            'memory_usage': 90.0,
            'response_time': 1500.0,
            'error_rate': 10.0,
            'requests_per_sec': 30.0
        }
        
        requests.post(f"{BASE_URL}/api/v1/metrics", json=anomaly_metric)
        time.sleep(3)
        
        # Check anomalies were detected
        response = requests.get(f"{BASE_URL}/api/v1/anomalies")
        anomalies = response.json()
        
        assert len(anomalies) > 0
        latest = anomalies[-1]
        assert 'anomaly_type' in latest
        assert 'severity' in latest
    
    def test_06_self_healing_trigger(self):
        """Test 6: Self-healing action triggered"""
        # Wait for healing action
        time.sleep(5)
        
        response = requests.get(f"{BASE_URL}/api/v1/healing-actions")
        actions = response.json()
        
        assert len(actions) > 0
        latest = actions[-1]
        assert 'action_type' in latest
        assert latest['status'] in ['completed', 'executing']
    
    def test_07_system_recovery(self):
        """Test 7: System recovers after healing"""
        time.sleep(10)
        
        response = requests.get(f"{BASE_URL}/api/v1/status")
        data = response.json()
        
        # Health should improve after healing
        assert data['health_score'] > 70
    
    def test_08_dashboard_accessible(self):
        """Test 8: Dashboard is accessible"""
        response = requests.get(f"{BASE_URL}/")
        assert response.status_code == 200
        assert 'Self-Healing Platform' in response.text
    
    def test_09_api_performance(self):
        """Test 9: API performance is acceptable"""
        endpoints = [
            '/api/v1/status',
            '/api/v1/metrics?limit=10',
            '/api/v1/anomalies?limit=10',
            '/api/v1/healing-actions?limit=10'
        ]
        
        for endpoint in endpoints:
            start = time.time()
            response = requests.get(f"{BASE_URL}{endpoint}")
            duration = time.time() - start
            
            assert response.status_code == 200
            assert duration < 1.0, f"{endpoint} took {duration:.2f}s (> 1.0s)"
    
    def test_10_concurrent_requests(self):
        """Test 10: Handle concurrent requests"""
        import concurrent.futures
        
        def make_request(i):
            response = requests.get(f"{BASE_URL}/api/v1/status")
            return response.status_code == 200
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request, i) for i in range(50)]
            results = [f.result() for f in futures]
        
        success_rate = sum(results) / len(results) * 100
        assert success_rate >= 95, f"Success rate: {success_rate}% (< 95%)"
```

### 3.2 Automated Testing Script

**File: `scripts/run_all_tests.sh`**

```bash
#!/bin/bash
set -e

echo "üß™ Running Complete Test Suite"
echo "=============================="

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Test results directory
RESULTS_DIR="test-results"
mkdir -p ${RESULTS_DIR}

# Timestamp
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

echo -e "${YELLOW}Step 1: Unit Tests${NC}"
pytest tests/unit/ -v --cov=src --cov-report=html --cov-report=xml \
    --html=${RESULTS_DIR}/unit_tests_${TIMESTAMP}.html --self-contained-html

echo -e "${YELLOW}Step 2: Integration Tests${NC}"
pytest tests/integration/ -v \
    --html=${RESULTS_DIR}/integration_tests_${TIMESTAMP}.html --self-contained-html

echo -e "${YELLOW}Step 3: E2E Tests${NC}"
pytest tests/e2e/ -v \
    --html=${RESULTS_DIR}/e2e_tests_${TIMESTAMP}.html --self-contained-html

echo -e "${YELLOW}Step 4: Performance Tests${NC}"
pytest tests/performance/ -v \
    --html=${RESULTS_DIR}/performance_tests_${TIMESTAMP}.html --self-contained-html

echo -e "${GREEN}‚úÖ All tests completed!${NC}"
echo ""
echo "üìä Test Reports:"
echo "  Coverage: htmlcov/index.html"
echo "  Unit Tests: ${RESULTS_DIR}/unit_tests_${TIMESTAMP}.html"
echo "  Integration: ${RESULTS_DIR}/integration_tests_${TIMESTAMP}.html"
echo "  E2E: ${RESULTS_DIR}/e2e_tests_${TIMESTAMP}.html"
echo "  Performance: ${RESULTS_DIR}/performance_tests_${TIMESTAMP}.html"
```

---

## üìä MONITORING SETUP

**File: `monitoring/prometheus/prometheus.yml`**

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - 'alerts.yml'
  - 'recording-rules.yml'

scrape_configs:
  - job_name: 'self-healing-platform'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names:
            - self-healing-platform
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: self-healing-platform
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
```

**File: `monitoring/prometheus/alerts.yml`**

```yaml
groups:
  - name: self_healing_platform
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: cpu_usage > 80
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for 2 minutes"
      
      - alert: HighMemoryUsage
        expr: memory_usage > 85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for 2 minutes"
      
      - alert: HighErrorRate
        expr: error_rate > 5
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is above 5% for 1 minute"
      
      - alert: AnomalyDetected
        expr: increase(anomalies_detected_total[5m]) > 0
        labels:
          severity: warning
        annotations:
          summary: "Anomaly detected by ML model"
          description: "ML model detected {{ $value }} anomalies in last 5 minutes"
      
      - alert: HealingActionFailed
        expr: healing_actions_failed_total > 0
        labels:
          severity: critical
        annotations:
          summary: "Self-healing action failed"
          description: "{{ $value }} healing actions have failed"
```

---

## üì¶ COMPLETE MAKEFILE

**File: `Makefile`**

```makefile
.PHONY: help install test deploy clean

help:
	@echo "AI Self-Healing Platform - Make Commands"
	@echo "=========================================="
	@echo ""
	@echo "Setup & Installation:"
	@echo "  make install          - Install all dependencies"
	@echo "  make install-jmeter   - Install JMeter"
	@echo "  make install-k8s      - Install Minikube"
	@echo ""
	@echo "Development:"
	@echo "  make run              - Run platform locally"
	@echo "  make test             - Run all tests"
	@echo "  make test-unit        - Run unit tests only"
	@echo "  make test-integration - Run integration tests"
	@echo "  make lint             - Run code linting"
	@echo ""
	@echo "Testing & Validation:"
	@echo "  make load-test        - Run JMeter load tests"
	@echo "  make chaos-test       - Run chaos engineering tests"
	@echo "  make test-all         - Run complete test suite"
	@echo ""
	@echo "Deployment:"
	@echo "  make docker-build     - Build Docker image"
	@echo "  make deploy-k8s       - Deploy to Kubernetes"
	@echo "  make deploy-aws       - Deploy to AWS EKS"
	@echo "  make deploy-azure     - Deploy to Azure AKS"
	@echo ""
	@echo "Monitoring:"
	@echo "  make logs             - View platform logs"
	@echo "  make status           - Check deployment status"
	@echo ""
	@echo "Cleanup:"
	@echo "  make clean            - Clean build artifacts"
	@echo "  make clean-all        - Clean everything including deployments"

install:
	pip install -r requirements-dev.txt

install-jmeter:
	./scripts/install_jmeter.sh

install-k8s:
	./scripts/install_minikube.sh

run:
	python src/api/main.py

test:
	pytest tests/ -v --cov=src

test-unit:
	pytest tests/unit/ -v

test-integration:
	pytest tests/integration/ -v

test-e2e:
	pytest tests/e2e/ -v

lint:
	black src/ tests/
	flake8 src/ tests/ --max-line-length=120
	mypy src/ --ignore-missing-imports

load-test:
	./jmeter/run_all_tests.py

chaos-test:
	./chaos/run_chaos_tests.py

test-all:
	./scripts/run_all_tests.sh

docker-build:
	docker build -t self-healing-platform:latest .

docker-run:
	docker run -p 8000:8000 self-healing-platform:latest

deploy-k8s:
	./scripts/deploy_to_kubernetes.sh

deploy-aws:
	./scripts/deploy_to_aws.sh

deploy-azure:
	./scripts/deploy_to_azure.sh

logs:
	kubectl logs -f deployment/self-healing-platform -n self-healing-platform

status:
	kubectl get all -n self-healing-platform

clean:
	rm -rf __pycache__
	rm -rf .pytest_cache
	rm -rf htmlcov
	rm -rf .coverage
	find . -type d -name "*.egg-info" -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete

clean-all: clean
	kubectl delete namespace self-healing-platform || true
	docker rmi self-healing-platform:latest || true
```

---

## üéØ EXECUTION CHECKLIST

### Phase 2 Checklist:

- [ ] Install JMeter: `make install-jmeter`
- [ ] Install Minikube: `make install-k8s`
- [ ] Run load tests: `make load-test`
- [ ] Run chaos tests: `make chaos-test`
- [ ] Deploy to K8s: `make deploy-k8s`
- [ ] Deploy to AWS: `make deploy-aws` (optional)

### Phase 3 Checklist:

- [ ] Setup GitHub Actions (copy `.github/workflows/`)
- [ ] Run unit tests: `make test-unit`
- [ ] Run integration tests: `make test-integration`
- [ ] Run E2E tests: `make test-e2e`
- [ ] Run complete suite: `make test-all`
- [ ] Setup monitoring (Prometheus + Grafana)

---

## üìù NEXT STEPS

1. **Week 1 (Jan 5-11)**: JMeter setup and load testing
2. **Week 2 (Jan 12-18)**: Chaos engineering implementation
3. **Week 3 (Jan 19-25)**: Kubernetes deployment (local)
4. **Week 4 (Jan 26 - Feb 1)**: Cloud deployment (AWS/Azure)
5. **Week 5-6 (Feb)**: CI/CD pipeline and automated testing
6. **Week 7-8 (Mar)**: Production hardening and optimization

---

**Would you like me to create:**
1. Additional specific scripts?
2. More detailed documentation?
3. Cloud provider-specific configurations?
4. Monitoring dashboard configurations?

Let me know what else you need!
